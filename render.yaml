services:
  - type: web
    name: ai-job-backend
    env: python
    rootDir: ai_job_backend
    # Skip Playwright (~150MB) when FREE_TIER=true: use paste + ScraperAPI only for demo
    buildCommand: pip install -r requirements.txt && ( [ "$FREE_TIER" = "true" ] || playwright install chromium )
    startCommand: uvicorn api.main:app --host 0.0.0.0 --port $PORT
    healthCheckPath: /health
    autoDeploy: true
    envVars:
      - key: PYTHON_VERSION
        value: "3.12"
      # Set FREE_TIER=true for demo to save RAM, skip Playwright, use paste/ScraperAPI
      - key: FREE_TIER
        value: "true"
      # Database: use DATABASE_URL OR PG_* (for Supabase, PG_* avoids password encoding)
      # Option A: DATABASE_URL (Render Postgres or Supabase - encode @ in password as %40)
      - key: DATABASE_URL
        sync: false
      # Option B: PG_* for Supabase - set these, leave DATABASE_URL empty
      - key: PG_HOST
        sync: false
      - key: PG_USER
        sync: false
      - key: PG_PASSWORD
        sync: false
      - key: PG_DATABASE
        sync: false
      - key: PG_PORT
        sync: false
      # Generate a secret in Dashboard (e.g. "Generate value") or set your own
      - key: OPENAI_API_KEY
        sync: false
      - key: AUTH_SECRET_KEY
        sync: false
      - key: FRONTEND_URL
        sync: false
      - key: ALLOWED_ORIGINS
        sync: false
      # Required for LinkedIn/Glassdoor scraping - get token at browserless.io (6 hrs free/mo)
      - key: BROWSERLESS_URL
        sync: false
      # Supabase: required for /api/job/rank-for-user and agent profile lookup
      - key: SUPABASE_URL
        sync: false
      - key: SUPABASE_SERVICE_ROLE_KEY
        sync: false
      # Azure/DeepSeek: optional override (defaults in config); set if not using OPENAI_BASE_URL from .env
      - key: OPENAI_BASE_URL
        sync: false
