services:
  - type: web
    name: ai-job-backend
    env: python
    rootDir: ai_job_backend
    # Skip Playwright (~150MB) when FREE_TIER=true: use paste + ScraperAPI only for demo
    buildCommand: pip install -r requirements.txt && ( [ "$FREE_TIER" = "true" ] || playwright install chromium )
    startCommand: uvicorn api.main:app --host 0.0.0.0 --port $PORT
    healthCheckPath: /health
    autoDeploy: true
    envVars:
      - key: PYTHON_VERSION
        value: "3.12"
      # Set FREE_TIER=true for demo to save RAM, skip Playwright, use paste/ScraperAPI
      - key: FREE_TIER
        value: "true"
      # Database: use DATABASE_URL OR PG_* (for Supabase, PG_* avoids password encoding)
      # Option A: DATABASE_URL (Render Postgres or Supabase - encode @ in password as %40)
      - key: DATABASE_URL
        sync: false
      # Option B: PG_* for Supabase - set these, leave DATABASE_URL empty
      - key: PG_HOST
        sync: false
      - key: PG_USER
        sync: false
      - key: PG_PASSWORD
        sync: false
      - key: PG_DATABASE
        sync: false
      - key: PG_PORT
        sync: false
      # Generate a secret in Dashboard (e.g. "Generate value") or set your own
      - key: OPENAI_API_KEY
        sync: false
      - key: AUTH_SECRET_KEY
        sync: false
      - key: FRONTEND_URL
        sync: false
      - key: ALLOWED_ORIGINS
        sync: false
      # Optional: Browserless (6 hrs free/month) - different IPs for LinkedIn/Glassdoor
      - key: BROWSERLESS_URL
        sync: false
      # Optional: ScraperAPI - production-grade job scraping, 1000 free credits/mo
      - key: SCRAPER_API_KEY
        sync: false
